{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19055100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Automated Resume Keyword Extractor\n",
    "Ms. Nishigandha Wankhade\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba69959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk     # Python library use for processing human language data\n",
    "import re       # Python's Regular Expressions (regex) module for text pattern matching\n",
    "            # used to clean text, remove unwanted characters, or exact specific patterns like email addresses\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize   # splits text into individual words (token) and splits text into sentences respectively\n",
    "from nltk.corpus import stopwords   # imports stopwords (like \"the\", \"is\", \"and\", etc) from nltk corpus\n",
    "from nltk import pos_tag, ne_chunk  # imports parts-of-speech (POS) tagging and Named Entity Recognition (NER) functions\n",
    "                        # pos-tag(tokens): assigns POS togs (e.g., noun, verb, adjective) to words\n",
    "                        # ne_chunk(tagged_words): performed NER to detect names, organizations, locations, etc.\n",
    "from nltk.tree import Tree    #  Tree class represents hierarchical structures like parse trees in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d111bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wankh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wankh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\wankh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\wankh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wankh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8a34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined list of skills\n",
    "SKILLS_LIST = {\"Python\", \"Machine Learning\", \"Data Science\", \"SQL\", \"Excel\", \"Tableau\", \"Power BI\", \n",
    "               \"Deep Learning\", \"TensorFlow\", \"NLP\", \"Snowflake\", \"AWS\", \"PyTorch\", \"Pandas\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb958e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    clean_tokens = []\\n    \\n    for word in tokens:         # to iterate through tokens to remove punctuation\\n        if word.isalnum():\\n            clean_tokens.append(word)  # to append only alphanumeric words to clean_tokens\\n            \\n    for word in clean_tokens:\\n        if word.lower() not in stopwords.words('english'):   # iterating over clean_tokens(), checking each word against stopwords.\\n            clean_tokens.append(word)\\n            \\n    return clean_tokens\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to Tokenizes and cleans text by removing punctuation and stopwords.\n",
    "    Input: a resume / text\n",
    "    Returns: tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for word in tokens:         # to iterate through tokens to remove punctuation\n",
    "        if word.isalnum():\n",
    "            clean_tokens.append(word)  # to append only alphanumeric words to clean_tokens\n",
    "            \n",
    "    for word in clean_tokens:\n",
    "        if word.lower() not in stopwords.words('english'):   # iterating over clean_tokens(), checking each word against stopwords.\n",
    "            clean_tokens.append(word)\n",
    "            \n",
    "    return clean_tokens\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "824772bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    \"\"\"\n",
    "    Function to extracts named entities such as Job Titles and Company Names.\n",
    "    Input : Text/ resume\n",
    "    Output: entities\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)   # split the input text into sentences\n",
    "    print(f\"\\n TOKENIZED SENTENCES : \\n {sentences}\\n\")\n",
    "    \n",
    "    entities = set()        # uses set to store extracted entities, it ensures unique values (no duplicates)\n",
    "    \n",
    "    for sentence in sentences:   # iterate through each sentence in the tokenized list\n",
    "        words = word_tokenize(sentence)  # splits the sentence into words (tokens)\n",
    "        print(\"\\n===============================================================================\")\n",
    "        print(f\"TOKENIZED WORDS: \\n {words} \\n\")\n",
    "        \n",
    "        tagged_words = pos_tag(words)    # assigns a part-of-speech (POS) tag (e.g., noun, verb, adjective) to each word\n",
    "        print(f\"TAGGED WORDS:\\n {tagged_words}\\n\")\n",
    "        \n",
    "        chunked_tree = ne_chunk(tagged_words) # identifies named entities from the POS-tagged words and build a tree structure to categorize entities (e.g. PERSON, ORGANIZATION).\n",
    "        print(f\"CHUNKED TREE : \\n {chunked_tree} \\n\")\n",
    "        \n",
    "        for subtree in chunked_tree:\n",
    "            if isinstance(subtree, Tree):     # iterate through the chunks in the tree\n",
    "                                            # checks if a chunk is a subtree (Tree), means it's a named entity\n",
    "                entity_name = \" \".join([token for token, pos in subtree.leaves()])  # extract actual word(tokens) from the subtree\n",
    "                                    # joins multi-word entities (e.g. \"New York\" instead of \"New\" and \"York\" separately)\n",
    "                    \n",
    "                entities.add(entity_name)\n",
    "                \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6f2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(tokens):\n",
    "    \"\"\"\n",
    "    A function to match extracted words with predefined skills list.\n",
    "    Input: tokens / words\n",
    "    Output : Matching skills\n",
    "    \n",
    "    \"\"\"\n",
    "    found_skills = set()\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in SKILLS_LIST:\n",
    "            found_skills.add(token)\n",
    "    return found_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba682e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# Load resume text (replace with actual text input)\n",
    "resume_text = \"\"\" John Doe is a Data Scientist with expertise in Machine Learning, Python, and SQL.\n",
    "He has worked at Google and Amazon, focusing on NLP and Deep Learning applications.\n",
    "He is proficient in Power BI and Snowflake for data analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess and extract information\n",
    "tokens = preprocess_text(resume_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ce4698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Doe', 'Data', 'Scientist', 'expertise', 'Machine', 'Learning', 'Python', 'SQL', 'worked', 'Google', 'Amazon', 'focusing', 'NLP', 'Deep', 'Learning', 'applications', 'proficient', 'Power', 'BI', 'Snowflake', 'data', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9d745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TOKENIZED SENTENCES : \n",
      " [' John Doe is a Data Scientist with expertise in Machine Learning, Python, and SQL.', 'He has worked at Google and Amazon, focusing on NLP and Deep Learning applications.', 'He is proficient in Power BI and Snowflake for data analysis.']\n",
      "\n",
      "\n",
      "===============================================================================\n",
      "TOKENIZED WORDS: \n",
      " ['John', 'Doe', 'is', 'a', 'Data', 'Scientist', 'with', 'expertise', 'in', 'Machine', 'Learning', ',', 'Python', ',', 'and', 'SQL', '.'] \n",
      "\n",
      "TAGGED WORDS:\n",
      " [('John', 'NNP'), ('Doe', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Data', 'NNP'), ('Scientist', 'NN'), ('with', 'IN'), ('expertise', 'NN'), ('in', 'IN'), ('Machine', 'NNP'), ('Learning', 'NNP'), (',', ','), ('Python', 'NNP'), (',', ','), ('and', 'CC'), ('SQL', 'NNP'), ('.', '.')]\n",
      "\n",
      "CHUNKED TREE : \n",
      " (S\n",
      "  (PERSON John/NNP)\n",
      "  (ORGANIZATION Doe/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  Data/NNP\n",
      "  Scientist/NN\n",
      "  with/IN\n",
      "  expertise/NN\n",
      "  in/IN\n",
      "  (GPE Machine/NNP)\n",
      "  Learning/NNP\n",
      "  ,/,\n",
      "  (PERSON Python/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (ORGANIZATION SQL/NNP)\n",
      "  ./.) \n",
      "\n",
      "\n",
      "===============================================================================\n",
      "TOKENIZED WORDS: \n",
      " ['He', 'has', 'worked', 'at', 'Google', 'and', 'Amazon', ',', 'focusing', 'on', 'NLP', 'and', 'Deep', 'Learning', 'applications', '.'] \n",
      "\n",
      "TAGGED WORDS:\n",
      " [('He', 'PRP'), ('has', 'VBZ'), ('worked', 'VBN'), ('at', 'IN'), ('Google', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), (',', ','), ('focusing', 'VBG'), ('on', 'IN'), ('NLP', 'NNP'), ('and', 'CC'), ('Deep', 'NNP'), ('Learning', 'NNP'), ('applications', 'NNS'), ('.', '.')]\n",
      "\n",
      "CHUNKED TREE : \n",
      " (S\n",
      "  He/PRP\n",
      "  has/VBZ\n",
      "  worked/VBN\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  and/CC\n",
      "  (GPE Amazon/NNP)\n",
      "  ,/,\n",
      "  focusing/VBG\n",
      "  on/IN\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  and/CC\n",
      "  (PERSON Deep/NNP Learning/NNP)\n",
      "  applications/NNS\n",
      "  ./.) \n",
      "\n",
      "\n",
      "===============================================================================\n",
      "TOKENIZED WORDS: \n",
      " ['He', 'is', 'proficient', 'in', 'Power', 'BI', 'and', 'Snowflake', 'for', 'data', 'analysis', '.'] \n",
      "\n",
      "TAGGED WORDS:\n",
      " [('He', 'PRP'), ('is', 'VBZ'), ('proficient', 'JJ'), ('in', 'IN'), ('Power', 'NNP'), ('BI', 'NNP'), ('and', 'CC'), ('Snowflake', 'NNP'), ('for', 'IN'), ('data', 'NN'), ('analysis', 'NN'), ('.', '.')]\n",
      "\n",
      "CHUNKED TREE : \n",
      " (S\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  proficient/JJ\n",
      "  in/IN\n",
      "  (GPE Power/NNP)\n",
      "  BI/NNP\n",
      "  and/CC\n",
      "  (PERSON Snowflake/NNP)\n",
      "  for/IN\n",
      "  data/NN\n",
      "  analysis/NN\n",
      "  ./.) \n",
      "\n",
      "NAMED ENTITIES : \n",
      " {'Google', 'Machine', 'Power', 'John', 'Deep Learning', 'Snowflake', 'Amazon', 'NLP', 'SQL', 'Python', 'Doe'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "named_entities = extract_named_entities(resume_text)\n",
    "print(f\"NAMED ENTITIES : \\n {named_entities} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70032494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLS MATCHED: \n",
      " {'NLP', 'SQL', 'Python', 'Snowflake'}\n"
     ]
    }
   ],
   "source": [
    "skills = extract_skills(tokens)\n",
    "print(f\"SKILLS MATCHED: \\n {skills}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fce803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Extracted Named Entities (Job Titles, Companies, etc.): {'Google', 'Machine', 'Power', 'John', 'Deep Learning', 'Snowflake', 'Amazon', 'NLP', 'SQL', 'Python', 'Doe'}\n",
      "\n",
      " Extracted Skills : {'NLP', 'SQL', 'Python', 'Snowflake'}\n"
     ]
    }
   ],
   "source": [
    "# Display All Extracted Information\n",
    "print(f\"\\n Extracted Named Entities (Job Titles, Companies, etc.): {named_entities}\")\n",
    "print(f\"\\n Extracted Skills : {skills}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scope for future development:\n",
    "\n",
    "1. Can convert the script into a web-based tool (Flask, Streamlit).\n",
    "2. Can improve different text format resume, e.g. PDF, Word documents by using pdfminer or python-docx.\n",
    "3. For more advance skills extraction, can use TF-IDF or spaCy\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
